# -*- coding: utf-8 -*-
"""PROJEK PREDICTIVE ANALYSIS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1arxtWIZ5aGerPoapIBHS28QvG7zo_wvs

#1. Import Library
"""

# @title import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""Pertama saya import library yang diperlukan untuk proses kedepannnya baik untuk data preparation maupun data understanding

#2. Data Loading
"""

# @title Data loading
df = pd.read_csv('https://drive.google.com/uc?id=1ThnWQI28p89ywd8UhLkki0st-2BwSDJm')
df

# @title melihat informasi dataset
df.info()

"""Melihat informasi dataset"""

# @title melihat apakah ada nilai kosong
df.isnull().sum()

"""Tidak terdapat missing value, sehingga tidak diperlukan penanganan lebih lanjut untuk missing value"""

# @title melihat apakah ada nilai duplikat
df.duplicated().sum()

"""Tidak terdapat nilai duplikat, sehingga tidak diperlukan penanganan lebh lanjut untuk data duplikat"""

# @title melihat 5 baris pertama dataset
df.head()

"""#3. Preprocessing"""

# @title menghapus kolom yang tidak digunakan yaitu "User ID"
df = df.drop(columns='User ID')
df.head()

"""Drop kolom User ID karena tidak relevan untuk model development dan eda"""

# @title melihat nilai unik setiap kolom kategorikal
for col in df.columns:
  if df[col].dtype == 'object':
    print(f"{col}: {df[col].unique()}")

"""Saya may melihat nilai unik tiap kolom kategorikal agar saya dapat menentukan metode encoded yang tepat untuk kolom masing-masing"""

# @title mengubah kolom kategorikal menjadi numerikal menggunakan label encoder
labencoder = preprocessing.LabelEncoder()
df['Operating System'] = labencoder.fit_transform(df['Operating System'])
df['Gender'] = labencoder.fit_transform(df['Gender'])
df

"""Menggunakan Label Encoder untuk kolom Operating System dan Gender untuk merubah nilai mereka menjadi vektor numerik."""

# @title One hot encoding untuk kolom kategorikal non ordinal
df = df.join(pd.get_dummies(df['Device Model'], dtype=int))
df = df.drop(columns='Device Model')

df

"""lalu selanjutnnya saya melakukkan one hot encoding untuk kolomh device model karena mereka adalah data kategorikal non ordinal"""

df.info()

"""#4. Data Understanding(Exploratory Data Analysis)"""

# @title Melihat Deskripsi statistik Data menggunakan fungsi describe()
df.describe()

"""setelah dianalisa didapatkan analisa statistik sebagai berikut:
1. **Operating System**: Kolom ini berisi nilai biner (0 atau 1) untuk tipe sistem operasi. Mean sebesar 0.21 menunjukkan bahwa mayoritas data memiliki nilai 0.
2. **App Usage Time**: Waktu rata-rata penggunaan aplikasi adalah 271.13 menit per hari, dengan standar deviasi sebesar 177.20 menit, menunjukkan adanya variasi yang signifikan di antara pengguna.
3. **Screen On Time**: Rata-rata waktu layar menyala adalah 5.27 jam per hari, dengan variasi yang relatif moderat.
4. **Battery Drain**: Penggunaan daya rata-rata adalah 1525.16 mAh per hari, dengan maksimum mencapai 2993 mAh.
5. **Age**: Rata-rata usia pengguna adalah 38.48 tahun, dengan kisaran usia 18â€“59 tahun.
6. **Gender**: Kolom biner yang hampir seimbang (0 untuk perempuan, 1 untuk laki-laki) dengan rata-rata 0.52.
7. **User Behavior Class**: Rata-rata kelas perilaku pengguna adalah 2.99, mendekati median (3), menunjukkan distribusi yang cukup seimbang.
"""

# @title Melihat distribusi User Behavior
ubehave_counts = df['User Behavior Class'].value_counts()
print(ubehave_counts)

plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='User Behavior Class')
plt.title('User Behavior Distribution')
plt.xlabel('Behavior Class')
plt.ylabel('number of individuals')
plt.show()

"""Disini saya mau melihat distribusi User behavior class untuk mengentahui apakah data inbalanced atau tida, dan didapatkan data ini cukup balance dan dapat digunakan"""

# @title visualisasi boxplot
numeric_columns = df.select_dtypes(include=[np.number]).columns

n_cols = 3
n_rows = len(numeric_columns) // n_cols + (1 if len(numeric_columns) % n_cols != 0 else 0)

plt.figure(figsize=(15, 5 * n_rows))

for i, col in enumerate(numeric_columns, start=1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(data=df, x=col)
    plt.title(f"Boxplot: {col}")
    plt.tight_layout()

plt.show()

"""setelah divisualisasikan, didapatkan analisa bahwa tidka ada outlier pada data, sehingga data bisa digunakan dengan baik untuk pelatihan model"""

# @title melihat visualisasi distribusi data
for col in numeric_columns:
    plt.figure(figsize=(14, 6))

    # Subplot untuk Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(df[col], kde=True, bins=30, color='skyblue', edgecolor='black')
    plt.title(f"Histogram of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")

    # Subplot untuk Boxplot
    plt.subplot(1, 2, 2)
    sns.boxplot(x=df[col], color='lightgreen')
    plt.title(f"Boxplot of {col}")
    plt.xlabel(col)

    plt.tight_layout()
    plt.show()

"""Dari visualisasi diatas, didapatkan bahwa distribusi data mayoritas simetrs sehingga memiliki rataan yang cukup baik"""

# @title visualisasi korelasi
plt.figure(figsize=(15, 10))
sns.heatmap(df.corr(), annot=True, cmap='Blues', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""Visualisasi heatmap korealasi untuk mengetahui hubungan antar fitur dan label, setalah divisualisasikan didapatkan insight sebagai berikut:
1. **Keterkaitan Antara Waktu Penggunaan, Waktu Layar, dan Variabel Lainnya**:  
   Variabel seperti **waktu penggunaan aplikasi**, **waktu layar**, **pengurasan baterai**, **jumlah aplikasi yang diinstal**, dan **penggunaan data** menunjukkan hubungan positif yang kuat. Semakin tinggi salah satu variabel, semakin tinggi pula variabel lainnya, mencerminkan pola penggunaan yang saling memengaruhi.

2. **Hubungan dengan Kategori Perilaku Pengguna**:  
   Terdapat korelasi positif antara **waktu penggunaan aplikasi**, **waktu layar**, dan **pengurasan baterai** dengan **kategori perilaku pengguna**. Pengguna dengan kebiasaan penggunaan serupa cenderung berada dalam kategori perilaku yang sama.
3. **Hubungan fitur yang terlalu kuat**:
   Terdapat fitur yang memiliki hubungan yang terlalu kuat, yaitu 1(operating system dan iphone 12), hal tersebut kemungkinan besar disebabkan oleh jenis iphone dan operating system yang sama ios, sehingga menimbulkan hubungan yang sama persis, hal tersebut tidak baik untuk model, karena dapat menyebabkan overfiting sehingga saya menggahapus kolom "Iphone 12"
"""

# @title menghapus kolom Iphone 12
df = df.drop(columns='iPhone 12')
df

"""Menghapus kolom "Iphone 12"""

# @title visualisasi kembali corelation heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(df.corr(), annot=True, cmap='Blues', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""Setelah dihapus, tidak ada fitur yang memiliki hubungan yang terlalu kuat lagi, sehingga model dapat terhindar dari resiko overfiting"""

# @title visualisasi pairplot
sns.set_theme(style="whitegrid", context="notebook", palette="muted")

plt.figure(figsize=(10, 8))
sns.pairplot(
    df,
    hue='User Behavior Class',  # Kolom kategori untuk warna
    diag_kind="kde",  # Distribusi menggunakan KDE
    markers=["o", "s", "D"],  # Marker untuk membedakan kategori
    plot_kws={"alpha": 0.6},  # Transparansi untuk mencegah overlap
    height=2.5  # mengatur ukuran setiap plot
)

# Menambahkan judul (opsional)
plt.suptitle("Pair Plot dengan Hue 'User Behavior Class'", y=1.02, fontsize=16)

# Menampilkan plot
plt.show()

"""Setelah dianalisa, diapatkan insight sebagai berikut:
1. **Operating System**: Terlihat sedikit konsentrasi kelas 1 dan 3 di bagian tengah distribusi dari fitur "Operating System". Meskipun demikian, hubungan ini tidak terlalu kuat.
   
2. **Age**: Ada sedikit indikasi bahwa kelas 2 dan 5 lebih banyak ditemukan di ujung bawah distribusi "Age", meskipun pola ini juga tidak terlalu jelas.
   
3. **Screen Size**: Tidak ada pola atau korelasi yang dapat terlihat antara "Screen Size" dengan **User Behavior Class**.
   
4. **Battery Drain**: Fitur "Battery Drain" terlihat hampir independen dari **User Behavior Class**, tanpa korelasi yang kuat.
   
5. **Number of Apps Installed**: Ada sedikit indikasi bahwa kelas 2 lebih sering muncul di rentang atas distribusi "Number of Apps Installed".
   
6. **Data Usage Efficiency**: Distribusi "Data Usage Efficiency" antar **User Behavior Class** tampaknya acak tanpa pola yang jelas.

#5. feature engineering
"""

# @title memisahkan  atribut independen(x) dan dependen(y)
x = df.drop(columns='User Behavior Class')
y = df['User Behavior Class']

"""saya memisahkan fitur independen(x) dan dependen(y)"""

# @title train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
y_train = y_train - 1
y_test = y_test - 1
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""spliting dataset"""

# @title normalisasi menggunakan MinMaxScaler
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

"""Normalisasi dengan MinMaxScaler untuk fitur independen"""

# @title Modeling menggunakan 9 model berbeda untuk mengentahui model terbaik untuk kasus ini
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score

# Inisialisasi model
models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Support Vector Machine (SVM)": SVC(),
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": GaussianNB(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "XGBoost": XGBClassifier(),
    "AdaBoost": AdaBoostClassifier()
}

# Menyimpan hasil evaluasi
results = []
predictions = {}

for name, model in models.items():
    print(f"\n=== {name} ===")

    # Melatih model
    model.fit(x_train, y_train)

    # Prediksi pada data uji
    y_pred = model.predict(x_test)

    # Menyimpan prediksi dan aktual
    predictions[name] = pd.DataFrame({
        "Actual": y_test,
        "Predicted": y_pred
    })

    # Menghitung akurasi
    acc = accuracy_score(y_test, y_pred)
    print(f"Akurasi: {acc:.2f}")

    # Menampilkan classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Menyimpan hasil untuk analisis lebih lanjut
    results.append({"Model": name, "Accuracy": acc})

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Visualisasi confusion matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
    plt.title(f"Confusion Matrix: {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()
results_df = pd.DataFrame(results)
print("\nRingkasan Hasil:")
print(results_df)

# Menampilkan prediksi dan aktual untuk tiap model
for model_name, df in predictions.items():
    print(f"\n=== Hasil Prediksi dan Aktual: {model_name} ===")
    print(df.head())

"""Menggunakan 9 algoritma machine learning, saya membandingkan peforma mereka dan setelah melakukan evaluasi terhadap berbagai model seperti **Decision Tree**, **Random Forest**, **Gradient Boosting**, dan **XGBoost**, didapatkan bahwa model **Random Forest** dan **XGBoost** memiliki performa terbaik dalam memprediksi kelas perilaku pengguna. Kedua model ini menunjukkan hasil yang lebih stabil dan akurat dalam klasifikasi dibandingkan dengan model lainnya seperti **K-Nearest Neighbors** dan **Naive Bayes**, yang tidak mendukung **feature importance** dan memberikan performa yang lebih rendah.

"""

# @title cross validation
from sklearn.model_selection import cross_val_score

results_cv = []

# Cross-validation untuk setiap model
for name, model in models.items():
    print(f"\n=== {name} ===")

    # Cross-validation
    cv_scores = cross_val_score(model, x_train, y_train, cv=5, scoring='accuracy')

    # Menghitung rata-rata dan deviasi standar
    mean_accuracy = np.mean(cv_scores)
    std_dev = np.std(cv_scores)

    print(f"Mean Accuracy: {mean_accuracy:.4f}")
    print(f"Standard Deviation: {std_dev:.4f}")
    results_cv.append({"Model": name, "Mean Accuracy": mean_accuracy, "Standard Deviation": std_dev})
results_df_cv = pd.DataFrame(results_cv)

print("\nRingkasan Hasil Cross-Validation:")
print(results_df_cv)
plt.figure(figsize=(10, 6))
sns.barplot(x="Model", y="Mean Accuracy", data=results_df_cv, ci="sd")
plt.title("Mean Accuracy per Model with Standard Deviation (Cross-Validation)")
plt.xticks(rotation=45)
plt.xlabel("Model")
plt.ylabel("Mean Accuracy")
plt.show()

"""Selanjutnnya saya melakukan cross-validation untuk mengenathui apakah model nya overfiting atau tidak, dan didapatkan hasil berikut:
Model dengan **mean accuracy** yang sangat tinggi dan **standard deviation** yang rendah, seperti **Decision Tree**, **Random Forest**, **SVM**, **Naive Bayes**, dan **Gradient Boosting**, menunjukkan stabilitas yang baik dan tidak cenderung overfitting. Sebaliknya, model **AdaBoost** dengan **standard deviation** yang cukup tinggi menunjukkan adanya variabilitas yang lebih besar dalam kinerjanya.
"""

def plot_feature_importance_all_models(models, x_train, y_train, top_n=10):
    if isinstance(x_train, np.ndarray):
        raise ValueError("x_train harus berupa DataFrame dengan nama kolom untuk menampilkan feature importance.")

    for name, model in models.items():
        print(f"\n=== Feature Importance untuk Model: {name} ===")
        # Latih model
        model.fit(x_train, y_train)

        # Cek apakah model mendukung feature_importances_
        if hasattr(model, "feature_importances_"):
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1][:top_n]

            # Buat DataFrame feature importance
            feature_importance_df = pd.DataFrame({
                "Feature": x_train.columns[indices],
                "Importance": importances[indices]
            })

            # Tampilkan DataFrame
            print(feature_importance_df)

            # Visualisasi
            plt.figure(figsize=(10, 6))
            sns.barplot(x="Importance", y="Feature", data=feature_importance_df, palette="viridis")
            plt.title(f"Top {top_n} Feature Importances for {name}")
            plt.xlabel("Relative Importance")
            plt.ylabel("Feature")
            plt.show()
        else:
            print(f"Model {name} tidak mendukung feature_importances_.")
feature_names = x.columns
x_train = pd.DataFrame(x_train, columns=feature_names)

plot_feature_importance_all_models(models, x_train, y_train, top_n=10)

"""Terakhir adalah melakukan Feature Importance untuk mengidentifikasi fitur mana yang paling berpengaruh pada prediksi, dan didapatkan hasil sebagai berikut:
- **Fitur yang paling berpengaruh** di semua model yang mendukung **feature_importances_** adalah fitur terkait dengan penggunaan perangkat, seperti **jumlah aplikasi yang diinstal**, **waktu penggunaan aplikasi**, dan **drainase baterai**. Hal ini menunjukkan bahwa model-model ini mengandalkan perilaku pengguna sehari-hari dalam menentukan kategori penggunaan perangkat.
- **Fitur yang kurang penting** adalah **gender**, **age**, dan **jenis perangkat**, yang tampaknya tidak berperan signifikan dalam prediksi kategori penggunaan perangkat.
- Model-model seperti **KNN**, **SVM**, **Logistic Regression**, dan **Naive Bayes** tidak mendukung fitur **feature_importances_**, tetapi mereka masih dapat digunakan untuk klasifikasi berdasarkan pola yang lebih kompleks antar fitur.
"""